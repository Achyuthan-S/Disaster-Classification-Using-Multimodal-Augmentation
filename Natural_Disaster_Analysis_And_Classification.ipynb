{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOvNnyOxbFfS"
   },
   "source": [
    "Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YmcYEuPYYt4j"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LSTM, Reshape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rb2hUJIHbLFU"
   },
   "source": [
    "Image Data Augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sN1KTl3RYt-s"
   },
   "outputs": [],
   "source": [
    "# Setting parameter for Image Data augumentation to the traing data\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "\n",
    "# Image Data augumentation to the testing data\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ld95Ys7lbJK7"
   },
   "source": [
    "Loading our data and performing data augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pxZLm_S1Yt7V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 742 images belonging to 4 classes.\n",
      "Found 198 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# Performing data augumentation to train data\n",
    "x_train = train_datagen.flow_from_directory(r'C:\\Users\\achyu\\Downloads\\Natural_Disasters_Analysis_And_Classification-main(1)\\Natural_Disasters_Analysis_And_Classification-main\\Project Files\\dataset\\train_set', target_size = (64, 64),\n",
    "                                            batch_size = 5, color_mode = 'rgb', class_mode = 'categorical')\n",
    "\n",
    "# Performing data augumentation to test data\n",
    "x_test = test_datagen.flow_from_directory(r'C:\\Users\\achyu\\Downloads\\Natural_Disasters_Analysis_And_Classification-main(1)\\Natural_Disasters_Analysis_And_Classification-main\\Project Files\\dataset\\test_set', target_size = (64, 64),\n",
    "                                          batch_size = 5, color_mode = 'rgb', class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gGXUyY31aZ6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achyu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LSTM, Reshape\n",
    "# Initialize the model\n",
    "classifier = Sequential()\n",
    "\n",
    "# First convolution layer and pooling\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Second convolution layer and pooling\n",
    "classifier.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattening layer (to convert 2D feature maps to 1D)\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Reshape the output to be compatible with LSTM\n",
    "# LSTM expects input shape (batch_size, time_steps, features), so reshape accordingly\n",
    "classifier.add(Reshape((1, -1)))  # Reshape to (1, number of features)\n",
    "\n",
    "# Add LSTM layer (RNN)\n",
    "classifier.add(LSTM(units=50, return_sequences=False))  # You can adjust units as needed\n",
    "\n",
    "# Fully connected layer\n",
    "classifier.add(Dense(units=128, activation='relu'))\n",
    "\n",
    "# Output layer - 4 units for 4 classes (adjust based on your number of classes)\n",
    "classifier.add(Dense(units=4, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmfe68TOa_yF"
   },
   "source": [
    "Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "soKPwjhYaZ0n"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b7fon0xa6tx"
   },
   "source": [
    "Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SRNKbTSgaZwK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achyu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.4131 - loss: 1.2743 - val_accuracy: 0.6414 - val_loss: 0.8604\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achyu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 93ms/step - accuracy: 0.6259 - loss: 0.8962 - val_accuracy: 0.7071 - val_loss: 0.8132\n",
      "Epoch 4/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 5/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 93ms/step - accuracy: 0.7563 - loss: 0.6286 - val_accuracy: 0.7121 - val_loss: 0.8025\n",
      "Epoch 6/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 95ms/step - accuracy: 0.8189 - loss: 0.4612 - val_accuracy: 0.7475 - val_loss: 0.5780\n",
      "Epoch 8/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 9/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 92ms/step - accuracy: 0.8607 - loss: 0.3648 - val_accuracy: 0.7626 - val_loss: 0.7355\n",
      "Epoch 10/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 11/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 92ms/step - accuracy: 0.9249 - loss: 0.2215 - val_accuracy: 0.7576 - val_loss: 0.8110\n",
      "Epoch 12/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 13/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 92ms/step - accuracy: 0.9290 - loss: 0.2111 - val_accuracy: 0.7525 - val_loss: 0.7720\n",
      "Epoch 14/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 15/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 92ms/step - accuracy: 0.9722 - loss: 0.0941 - val_accuracy: 0.7525 - val_loss: 0.9260\n",
      "Epoch 16/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 17/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 92ms/step - accuracy: 0.9736 - loss: 0.0669 - val_accuracy: 0.7273 - val_loss: 1.1375\n",
      "Epoch 18/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 19/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 95ms/step - accuracy: 0.9776 - loss: 0.0485 - val_accuracy: 0.7980 - val_loss: 0.9593\n",
      "Epoch 20/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x258cfbe0d00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "classifier.fit(\n",
    "    x_train,  # Training data generator\n",
    "    steps_per_epoch=len(x_train),  # Number of steps per epoch\n",
    "    epochs=20,  # Number of epochs\n",
    "    validation_data=x_test,  # Validation data generator\n",
    "    validation_steps=len(x_test)  # Number of steps for validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.0082 - val_accuracy: 0.7778 - val_loss: 0.9625\n",
      "Epoch 2/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.7828 - val_loss: 1.0022\n",
      "Epoch 4/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 5/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.7778 - val_loss: 1.0145\n",
      "Epoch 6/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.7929 - val_loss: 1.0325\n",
      "Epoch 8/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 9/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 9.1893e-04 - val_accuracy: 0.7828 - val_loss: 1.0493\n",
      "Epoch 10/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 11/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 6.9384e-04 - val_accuracy: 0.7828 - val_loss: 1.0758\n",
      "Epoch 12/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 13/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 5.9153e-04 - val_accuracy: 0.7828 - val_loss: 1.0970\n",
      "Epoch 14/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 15/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 6.2264e-04 - val_accuracy: 0.7778 - val_loss: 1.1201\n",
      "Epoch 16/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 17/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 4.6069e-04 - val_accuracy: 0.7828 - val_loss: 1.1293\n",
      "Epoch 18/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 19/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 4.1576e-04 - val_accuracy: 0.7828 - val_loss: 1.1468\n",
      "Epoch 20/20\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "   accuracy  val_accuracy\n",
      "0       1.0      0.777778\n",
      "1       0.0      0.782828\n",
      "2       1.0      0.777778\n",
      "3       0.0      0.792929\n",
      "4       1.0      0.782828\n",
      "5       0.0      0.782828\n",
      "6       1.0      0.782828\n",
      "7       0.0      0.777778\n",
      "8       1.0      0.782828\n",
      "9       0.0      0.782828\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Train the model and store the history\n",
    "history = classifier.fit(\n",
    "    x_train,  # Training data generator\n",
    "    steps_per_epoch=len(x_train),  # Number of steps per epoch\n",
    "    epochs=20,  # Number of epochs\n",
    "    validation_data=x_test,  # Validation data generator\n",
    "    validation_steps=len(x_test)  # Number of steps for validation\n",
    ")\n",
    "\n",
    "# Check lengths of the lists\n",
    "train_accuracy = history.history.get('accuracy', [])\n",
    "val_accuracy = history.history.get('val_accuracy', [])\n",
    "\n",
    "# Ensure the lengths match\n",
    "min_length = min(len(train_accuracy), len(val_accuracy))\n",
    "\n",
    "# Create a DataFrame with the same length\n",
    "history_df = pd.DataFrame({\n",
    "    'accuracy': train_accuracy[:min_length],\n",
    "    'val_accuracy': val_accuracy[:min_length]\n",
    "})\n",
    "\n",
    "# Print the training and validation accuracy in a table format\n",
    "print(history_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "CQdhZLCdayQB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "classifier.save('disasternewcnnandrnn.h5')\n",
    "model_json = classifier.to_json()\n",
    "\n",
    "with open(\"model-bw.jsonnewcnnandrnn\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ijJH04eVaz5G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"disasternewcnnandrnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uR4FGYg4a1ig"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n"
     ]
    }
   ],
   "source": [
    "img = image.load_img(r\"C:\\Users\\achyu\\Downloads\\Natural_Disasters_Analysis_And_Classification-main(1)\\Natural_Disasters_Analysis_And_Classification-main\\Project Files\\dataset\\test_set\\Earthquake\\1322.jpg\", target_size = (64, 64))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis = 0)\n",
    "\n",
    "pred = np.argmax(model.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "xiz8BZp2a21D"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Earthquake'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = ['Cyclone', 'Earthquake', 'Flood', 'Wildfire']\n",
    "result = index[pred]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
